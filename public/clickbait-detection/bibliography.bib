% Encoding: UTF-8

@Article{Hochreiter1997,
  author    = {Sepp Hochreiter and JÃ¼rgen Schmidhuber},
  title     = {Long Short-Term Memory},
  journal   = {Neural Computation},
  year      = {1997},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  month     = {nov},
  doi       = {10.1162/neco.1997.9.8.1735},
  publisher = {{MIT} Press - Journals},
}

@Article{Christopher2015,
  author    = {Christopher Olah},
  title     = {Understanding LSTM Networks},
  year      = {2015},
  timestamp = {2019-05-19},
  url       = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
}

@Article{Kingma2014,
  author      = {Diederik P. Kingma and Jimmy Ba},
  title       = {Adam: A Method for Stochastic Optimization},
  abstract    = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  date        = {2014-12-22},
  eprint      = {http://arxiv.org/abs/1412.6980v9},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1412.6980v9:PDF},
  keywords    = {cs.LG},
}

@Article{Ruder2016,
  author      = {Sebastian Ruder},
  title       = {An overview of gradient descent optimization algorithms},
  abstract    = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  date        = {2016-09-15},
  eprint      = {http://arxiv.org/abs/1609.04747v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1609.04747v2:PDF},
  keywords    = {cs.LG},
}

@Article{Keras,
  title     = {Keras: The Python Deep Learning library},
  year      = {2019},
  timestamp = {2019-05-19},
  url       = {https://keras.io/},
}

@Article{ClickbaitDataset2016,
  title = {Clickbait Challenge: Dataset},
  year  = {2017},
  url   = {https://www.clickbait-challenge.org/},
}

@Article{Chopra2017,
  author = {S. Chopra, S. Jain, J. M. Sholar.},
  title  = {Towards automatic identification of fake news: Headline-article stance detection with lstm attention models},
  year   = {2017},
}

@Article{Joulin2016,
  author      = {Armand Joulin and Edouard Grave and Piotr Bojanowski and Tomas Mikolov},
  title       = {Bag of Tricks for Efficient Text Classification},
  abstract    = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.},
  date        = {2016-07-06},
  eprint      = {http://arxiv.org/abs/1607.01759v3},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1607.01759v3:PDF},
  keywords    = {cs.CL},
}

@Article{Mikolov2013,
  author      = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  title       = {Efficient Estimation of Word Representations in Vector Space},
  abstract    = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  date        = {2013-01-16},
  eprint      = {http://arxiv.org/abs/1301.3781v3},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1301.3781v3:PDF},
  keywords    = {cs.CL},
}

@Article{Clickbait2016,
  title = {Clickbait Challenge},
  year  = {2017},
  url   = {https://www.clickbait-challenge.org/#data},
}

@Comment{jabref-meta: databaseType:bibtex;}
